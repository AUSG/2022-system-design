# 6장 : 키-값 저장소 설계

> **단일 서버 키-값 저장소를 설계한다면?**
 
 - 키-값 쌍 모두를 **해시테이블로 메모리 상에 저장**하는 것이 가장 직관적인 방법
    → 빠른 속도를 보장하지만, 모든 데이터가 메모리에 있는 것이 불가능할 수 있음
    → 데이터 압축 + 자주 쓰이는 데이터만 메모리 / 나머지는 디스크 저장으로 개선 가능
 → 하지만 언젠가 서버 한대로는 부족하다
    → 결론: **분산 키-값 저장소 필요**! 
 
> ** 분산 키-값 저장소 == 분산 해시 테이블**

- 분산 시스템을 설계하기 위한 CAP 정리

1. Consistency 일관성
	- 접속한 모든 클라이언트가 `어떤 노드에 접속했느냐에 관계없이` **같은 데이터**를 봐야 함
2. Availability 가용성
	- 접속한 클라이언트는 일부 노드에 `장애가 발생해도` **응답받을 수 있어야 함**
3. Partition Tolerance 파티션 감내
	- 파티션 : 두 노드 사이의 통신 장애 발생
    - 네트워크에 `파티션이 생겨도` **시스템은 계속 동작해야 함**
    
![](https://velog.velcdn.com/images/eunz_juu/post/e94dbc27-cd8f-4e80-b59b-4828ef55df71/image.png)
- 어떤 2가지를 충족하려면 1가지는 반드시 희생된다~ 

- CA 시스템 = 파티션 감내를 지원하지 않는데, 네트워크 장애는 불가피한 일이기에 분산 시스템은 반드시 !!! 파티션 문제 감내하도록 설계되어야 함
	
    - 관계형 데이터베이스가 동시에 다량의 서버를 운용하는 클러스터링에 적합하지 않는 이유가 여기에 있다 (P - 파티션 문제 감내 고려하여 설계되어있지 않음)

1. CP 시스템 선택하는 경우 (가용성 포기)
- 같은 데이터를 봐야 하므로, 정상 작동되는 노드에 대해 쓰기 연산 중단 시켜야 함.
→ 그러므로 가용성이 깨지게 됨
→ 은행권 시스템은 보통 일관성 포기하지 않으므로, 네트워크 파티션으로 인해 일관성이 깨질 우려가 있는 상황에서는 상황 해결 시까지는 오류 반환

2. AP 시스템 선택하는 경우 (일관성 포기)
- 낡은 데이터를 반환하더라도 읽기 연산을 허용 
- 파티션 문제가 해결된 후 새 데이터를 다른 노드에 전송하게 됨

> **시스템 컴포넌트**

**1. 데이터 파티션**
- 데이터를 파티션 단위로 분할하여 여러 대의 서버에 저장하자
- 파티션 단위로 나눌 때 따져봐야 할 점
	1. 데이터를 여러 서버에 고르게 분산 가능?
	2. 노드 추가/삭제 시 데이터 이동 최소화 가능?
→ 안정해시가 적합한 기술이다!
→ 시스템 부하에 따른 `규모 확장 자동화`, 다양성:`서버 용량에 맞게 가상노드 수 조정 가능`


**2. 데이터 다중화**
- 높은 가용성과 안정성 확보를 위해 **데이터를 N개 서버에 비동기적으로 다중화**해야 한다.
- 어떤 키를 해시 링 위에 배치하고, 시계방향으로 링을 순회하며 만나는 첫 N개의 서버에 데이터 사본 보관
→ `가상 노드` 사용 시, 선택한 **N개의 노드가 대응될 실제 물리서버 개수가 N보다 작아질 수 있다**
→ 따라서 노드 선택 시 같은 물리서버 `중복선택하지 않도록`
- 안정성을 담보하기 위해 데이터의 사본은 다른 센터 서버에 보관, 센터들은 고속네트워크로 연결

**3. 데이터 일관성**
- 정족수 합의 프로토콜 사용 시 **읽기/쓰기 연산에 모두 일관성 보장 가능**<br>: 정족수(定足數)는 여러 사람의 합의로 운영되는 의사기관에서 의결을 하는데 필요한 최소한의 참석자 수

![](https://velog.velcdn.com/images/eunz_juu/post/65a3b95d-336e-43fb-a0cb-15a6b6be6f3a/image.png)

N=사본 개수
R=읽기 연산에 대한 정족수
W=쓰기 연산에 대한 정족수 
→ 쓰기 연산 성공 간주되려면, 적어도 W개 서버로부터 쓰기 연산 성공했다는 응답 받아야 함
→ s1으로부터 성공 응답 받은 경우, s0+s2의 응답은 기다릴 필요X

중재자 : 클라이언트, 노드 사이에서 프록시 역할

W,R,N 값을 정하는 것 = 응답 지연, 데이터 일관성 사이의 타협점을 찾는 과정
	- W,R 값이 1보다 큰 경우 일관성 수준 향상 / 가장 느린 서버로부터 응답 대기 -> 응답 지연
	- W,R 값이 1인 경우 응답속도 빠름 / 일관성 보장X
    
W+R>N 인 경우, 강한 일관성이 보장된다.

- 일관성 모델 : 데이터 일관성 수준을 결정하는 중요한 요소
	
    - 강한 일관성 : 모든 읽기 연산은 **가장 최근 갱신된 결과 반환**
    	- 모든 사본에 현재 쓰기 연산 결과가 반영될 때까지 데이터에 대한 읽기/쓰기 금지
        → 새로운 요청 처리가 중단되므로 고가용성 시스템에는 부적합 
    - 약한 일관성 : 읽기 연산은 **가장 최근 갱신 결과를 반환하지 못할 수도 있음**
    - 최종 일관성 : 약한 일관성의 형태로, 갱신 결과가 결국에는 모든 사본에 반영되는 모델
    	- 다이나모, 카산드라가 택한 모델 
        - 하지만 최종 일관성 모델은 쓰기 연산이 `병렬적`으로 발생하기에 저장된 값의 일관성이 깨질 수 있는데, 이는 클라이언트가 해결해야 함
        → 클라이언트 측에서 **데이터의 버전 정보를 활용해** 일관성이 깨진 데이터를 읽지 않도록 한다.
        



**비 일관성 해소 기법 : 데이터 버저닝과 벡터 시계**
- **버저닝** : 데이터 변경할 때마다 해당 데이터의 새로운 버전을 만들며, 각 버전 데이터는 변경 불가

- **벡터 시계** : 충돌을 발견하고 자동으로 해결해 낼 버저닝 시스템
→ `어떤 버전이 선행인지/후행인지`, 다른 버전과 `충돌이 있는지` 판별하는데 쓰임
- [서버, 버전]의 순서쌍을 데이터에 매단 것
ex) D([Sn,Vn])  / D = 데이터 / Si = 서버 번호 / Vi = 버전 카운터

![](https://velog.velcdn.com/images/eunz_juu/post/a56ca651-e799-4b12-9a23-3d9f289c3a75/image.png)
**2번)** 다른 클라이언트가 데이터 D1을 읽고 D2로 업데이트한 다음 기록한다.
이 때, D2는 D1에 대한 변경이므로 D1을 덮어쓰는 것
**5번)** 클라이언트가 D3, D4를 읽으면 데이터간 충돌이 있다는 것을 알게됨 
→ D2 데이터를 Sy와 Sz 서버가 각기 다른 값으로 바꾸었으므로 
→ 충돌은 클라이언트가 해소한 후 서버에 기록

- 벡터 시계 사용하면, <u>버전X가 버전 Y의 이전 버전인지</u> 쉽게 판단 가능
→ 버전 Y에 포함된 모든 구성요소 값이 X에 포함된 구성요소 값보다 **같거나 큰지 확인**하면, 충돌이 없다는 것
ex) D([S0,1], [S1,1]) 은 D([S0,1], [S1,2]) 의 이전버전 
    = 두 데이터 간 충돌은 없다
    
- 버전 X,Y 사이에 충돌이 있는지 확인하려면 (= 두 버전이 같은 `이전 버전에서 파생된` 다른 버전인지 확인하려면) 버전 Y에 포함된 구성요소 중 X 구성요소보다 작은 값 갖는 것이 있는지 보면 됨

ex) D([S0,1], [S1,2]) 과 D([S0,2], [S1,1]) 
   = 충돌
   
- 벡터 시계를 사용해 충돌 감지/해소 방법의 단점
	1. 클라이언트 구현 복잡 - 충돌 감지/해소 로직이 클라이언트에 있어서
    2. [서버:버전] 순서쌍 개수가 빠르게 늘어남 
    → 임계치 설정, 임계치 이상으로 길이가 길어지면 오래된 순서쌍을 벡터 시계에서 제거하도록 함
    → 하지만, 이 경우 버전 선후 관계가 정확히 결정될 수 없어서 충돌 해소 과정의 효율성이 낮아짐
    → 아마존에서는 실제 그런 문제를 발견한 적이 없다고하니 써도 될 것 같다 
  
  
> 장애 처리하기

- 장애 처리
	- 보통 한 대 서버가 A 서버가 죽었다고 알린다고 해서 A서버를 바로 장애처리하지 않음
    → 2대 이상의 서버가 똑같이 A 서버의 장애를 보고해야 해당 서버에 실제 장애가 발생했다고 간주함
- 장애 감지    
	- 모든 노드 사이에 멀티캐스팅 채널 구축하는 것이 가장 쉬운 장애 감지 방법 
    → 서버가 많아지면 비효율적
    → 가십 프로토콜과 같은 분산형 장애 감지 솔루션 채택하는 것이 효율적

가십 프로토콜 (Gossip Protocol) : 분산 환경에서 메시지를 전달하는 커뮤니케이션 방식의 하나
- 소문이 전파되어나가듯 Broadcast 해주는 마스터가 없이 각 노드가 주기적으로 TCP/UDP 기반으로 메타데이터를 주고받으면서 데이터를 전송

![](https://velog.velcdn.com/images/eunz_juu/post/230d9cb9-6402-4a60-b32d-a9d851641394/image.png)

**`가십프로토콜`** 동작 원리
1. 각 노드는 멤버십 목록 (멤버ID, 박동 카운터) 쌍을 가짐
2. 노드는 주기적으로 자신의 박동 카운터 증가시킴
3. 각 노드는 무작위로 선정된 노드들에게 주기적으로 자신의 박동 카운터 목록을 보냄
4. 박동 카운터 목록을 받은 노드는 멤버십 목록을 최신 값으로 갱신
5. 어떤 멤버의 박동 카운터 값이 **지정된 시간 동안 갱신되지 않으면** 해당 멤버는 장애 상태인 것으로 간주

- 일시적 장애 처리
	
    - 네트워크, 서버문제로 장애 상태인 서버로 가는 요청은 다른 서버가 잠시 맡아 처리
    - 그동안 발생한 변경사항은 해당 서버 복구 시 일괄 반영 하여 `일관성` 보존
    → `단서 후 임시 위탁` 기법

- 영구적 장애 처리
	
    - 반-엔트로피 프로토콜 (사본들을 비교하여 최신 버전으로 갱신하는 과정 포함) 구현하여 사본을 동기화
    - 사본 간의 일관성이 망가진 상태를 탐지하고 전송 데이터 양을 줄이려면 머클 트리를 사용해야 함
   `머클 트리` 란, 각 노드에 그 자식 노드들에 보관된 값의 해시/또는 자식 노드들의 레이블로부터 계산된 해시 값을 레이블로 붙여두는 트리
 
![](https://velog.velcdn.com/images/eunz_juu/post/ea55e86d-2a76-4054-adda-6adfad36f0ea/image.png)
1. 키 공간을 **버킷으로 나눈다.**

![](https://velog.velcdn.com/images/eunz_juu/post/7b709538-97c6-4306-8c23-32e79b63784b/image.png)
2. 버킷에 포함된 각각의 키에 균등 분포 해시 함수 적용하여 **해시값 계산**

![](https://velog.velcdn.com/images/eunz_juu/post/6ae7c46e-cc7c-4fa4-8f5b-9a9e6d7d945a/image.png)
3. 버킷별로 해시값 계산한 후, **해당 해시 값을 레이블로 갖는 노드 생성**

![](https://velog.velcdn.com/images/eunz_juu/post/deff079c-3b31-4b18-9a2b-250bad00e01a/image.png)
4. 자식 노드의 레이블로부터, **새로운 해시 값 계산** 하여 상향식 이진트리 구성

- 2개의 머클트리 비교는 루트 노드의 해시값을 비교하는 것으로 시작
→ 해시 값이 일치한다면, 두 서버는 동일 데이터 가짐
→ 값이 다르다면, 왼쪽/오른쪽 자식노드의 해시 값 비교
→ 아래쪽으로 탐색해가면서 **다른 데이터를 갖는 버킷을 찾고 그 버킷들만 동기화**

머클 트리를 사용했을 때, 동기화 해야 하는 데이터 양은 
// 실제 존재하는 차이의 크기에 비례
// 두 서버에 보관된 데이터 총량과는 무관

- 데이터 센터 장애 처리
	- 데이터 센터 장애에 대응가능한 시스템을 만들기 위해 여러 데이터 센터에 다중화가 필요

- 쓰기 경로
![](https://velog.velcdn.com/images/eunz_juu/post/eb0bdf11-a95e-4f0c-a2a9-17baea85d33b/image.png)
1. 쓰기 요청이 커밋로그에 저장
2. 데이터가 메모리 캐시에 기록
3. 메모리 캐시 가득차거나 임계치에 도달한 경우, 데이터는 디스크에 있는 SSTable에 기록
SSTable (Sorted-String Table) : <키,값> 순서쌍을 정렬된 리스트 형태로 관리하는 테이블
    
- 읽기 경로
![](https://velog.velcdn.com/images/eunz_juu/post/33d5268a-4ae2-408e-8dd0-8c27d4b42e41/image.png)
1. 읽기 요청을 받으면 데이터가 메모리 캐시에 존재하는지부터 확인
2. 메모리 캐시에 있는 경우 바로 결과를 반환

![](https://velog.velcdn.com/images/eunz_juu/post/8d20389e-6b58-48bc-a423-86a9900289ba/image.png)
1. 읽기 요청을 받으면 데이터가 메모리 캐시에 존재하는지부터 확인
2. 메모리 캐시에 없는 경우 블룸 필터를 검사한다
3. 블룸 필터를 통해 어떤 SSTable에 찾는 키가 있는지 알아 낸다.
4. SSTable에서 데이터를 가져온다
5. 해당 데이터를 클라이언트에게 반환

> **정리 : 분산 키-값 저장소가 가져야 하는 기능, 기능 구현에 이용되는 기술 정리**

- 대규모 데이터 저장
    - 안정 해시를 사용해 서버들에 부하 분산  
- 읽기 연산에 대한 높은 가용성 보장
    - 데이터를 여러 데이터센터에 다중화
- 쓰기 연산에 대한 높은 가용성 보장
    - 버저닝, 벡터 시계를 사용한 충돌 해소
- 데이터 파티션 / 점진적 규모 확장성 / 다양성
    - 안정 해시
- 조절 가능한 데이터 일관성
    - 정족수 합의
- 일시적 장애 처리
    - 느슨한 정족수 프로토콜, 단서 후 임시 위탁
- 영구적 장애 처리
    - 머클 트리
- 데이터 센터 장애 대응
    - 여러 데이터 센터에 걸친 데이터 다중화